<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
    <h1>Text Preprocessing</h1>

<pre>
from google.colab import drive
drive.mount('/content/drive')
</pre>

<br>

<pre>
import nltk
nltk.download('punkt')
nltk.download('stopwords') 
nltk.download('omw-1.4')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
</pre>



<pre>
import nltk 
import string
from nltk.tokenize import word_tokenize
from nltk import sent_tokenize
import re

#Open the text file to apply the text preprocessing
#file = open("/content/gdrive/MyDrive/Colab_Notebooks/Jan 2022 - June 2022/NLP (AI & DS)/text1.txt","rt")
#text=file.read()
#file.close()
#text="The NLP is a domain which is a pat of AI"
text="NLP primarily comprises of  natural language understanding (human to machine) and natural language generation (machine to human). NLU aids in extracting valuable information from text such as social media data, customer surveys, and complaints."
print("The Text contents")
print(text)

print("------------------------------------------------")


#-------------------------------------------------
sentences = sent_tokenize(text)
print("After Sentence Segmentaton \n",sentences)
#text=sentences
print(len(sentences))
print("------------------------------------------------")
#-------------------------------------------------

tokens=word_tokenize(text)
print("Tokens")
print(tokens)
print(len(tokens))
print("------------------------------------------------")

#----------------------------------------------------
text=text.lower()
print("After converting to Lower cases")
print(text)
tokens=word_tokenize(text)
print("------------------------------------------------")


#---------------------------------------------------
num_remo = re.sub(r'\d+', '', text)
print("After removal of Numbers")
print(num_remo)
print("------------------------------------------------")

#-------------------------------------------------
translator = str.maketrans('', '', string.punctuation) 
text1= text.replace('-', ' ')
pun_rem=text1.translate(translator)
print("After Punctuation removal")
print(pun_rem)
print("------------------------------------------------")
</pre>





<pre>
!pip install inflect
</pre>




<pre>
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
</pre>


<pre>
import inflect
p=inflect.engine()
print(sentences)
sent=[]
new_str=[]
for line in sentences:
  temp=line.split()
 
  for word in temp:
    if word.isdigit():
      t=p.number_to_words(word)
      new_str.append(t)
    else:
      new_str.append(word)
  final_line=" ".join(new_str)
print(final_line)
</pre>


<pre>
print("------------------------------------------------")
#--------------------------------------------------
lemmatizer=WordNetLemmatizer()
print(tokens)
lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in tokens])
print("After Lemmatizing on Noun -->",lemmatized_output)
lemmatized_output=word_tokenize(lemmatized_output)
lemmatized_output_1 =  ' '.join([lemmatizer.lemmatize(w,'v') for w in lemmatized_output])
print("After Lemmatizing ")
print(lemmatized_output_1)
print("------------------------------------------------")
</pre>



<pre>
from nltk.stem import PorterStemmer 
from nltk.stem import LancasterStemmer

porter = PorterStemmer()
lancaster=LancasterStemmer()
#proide a word to be stemmed
print("Porter Stemmer")
ps = ' '.join([porter.stem(w) for w in tokens])
print(ps)

ps = ' '.join([lancaster.stem(w) for w in tokens])
print(ps)
</pre>

<pre>
from nltk.corpus import stopwords 
stop_words = set(stopwords.words('english'))
print(stop_words)
print(len(stop_words))
</pre>


<pre>
tokens=word_tokenize(lemmatized_output_1)
filtered_sentence = [w for w in tokens if not w in stop_words]
print("After Removing the stop words")
print(filtered_sentence)
</pre>


<pre>
from nltk.corpus import stopwords
print(stopwords.words('english'))
</pre>
</body>
</html>